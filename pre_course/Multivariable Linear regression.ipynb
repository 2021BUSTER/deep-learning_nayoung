{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd676087",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee3be2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[73,80,75],[93,88,93],[89,91,80],[96,98,100],[73,66,70]])\n",
    "y_train = torch.FloatTensor([[152],[185],[180],[196],[142]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a93b25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 초기화\n",
    "W = torch.zeros((3, 1), requires_grad = True)\n",
    "b = torch.zeros(1, requires_grad = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55db8011",
   "metadata": {},
   "source": [
    "### Hypothesis Function은 인공 신경망의 구조를 나타냄.  \n",
    "입력변수가 3개라면 weighteh 3개!\n",
    "\n",
    "H(x) = w1 * x1 + w2 * x2 + w3 * x3 + b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df38c428",
   "metadata": {},
   "source": [
    "### x가 많아질수록 식이 길어짐  \n",
    "    => matmul()로 한번에 계산  \n",
    "a. 더 간결하고,  \n",
    "b. x의 길이가 바뀌어도 코드를 바꿀 필요가 없고  \n",
    "c. 속도도 더 빠르다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bc940f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD([W, b],lr = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50c2da14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 hypothesis: tensor([154.0506, 185.1062, 175.7810, 198.5981, 141.2193]), Cost: 5.875126\n",
      "Epoch    1/20 hypothesis: tensor([154.0505, 185.1060, 175.7821, 198.5975, 141.2193]), Cost: 5.872619\n",
      "Epoch    2/20 hypothesis: tensor([154.0503, 185.1057, 175.7831, 198.5969, 141.2194]), Cost: 5.870029\n",
      "Epoch    3/20 hypothesis: tensor([154.0502, 185.1054, 175.7842, 198.5963, 141.2195]), Cost: 5.867534\n",
      "Epoch    4/20 hypothesis: tensor([154.0500, 185.1051, 175.7852, 198.5957, 141.2195]), Cost: 5.864966\n",
      "Epoch    5/20 hypothesis: tensor([154.0498, 185.1048, 175.7863, 198.5952, 141.2196]), Cost: 5.862443\n",
      "Epoch    6/20 hypothesis: tensor([154.0497, 185.1045, 175.7873, 198.5946, 141.2196]), Cost: 5.859941\n",
      "Epoch    7/20 hypothesis: tensor([154.0495, 185.1042, 175.7884, 198.5940, 141.2197]), Cost: 5.857362\n",
      "Epoch    8/20 hypothesis: tensor([154.0494, 185.1039, 175.7894, 198.5934, 141.2198]), Cost: 5.854857\n",
      "Epoch    9/20 hypothesis: tensor([154.0492, 185.1037, 175.7905, 198.5928, 141.2198]), Cost: 5.852319\n",
      "Epoch   10/20 hypothesis: tensor([154.0491, 185.1034, 175.7915, 198.5922, 141.2199]), Cost: 5.849789\n",
      "Epoch   11/20 hypothesis: tensor([154.0489, 185.1031, 175.7926, 198.5916, 141.2199]), Cost: 5.847253\n",
      "Epoch   12/20 hypothesis: tensor([154.0488, 185.1028, 175.7936, 198.5911, 141.2200]), Cost: 5.844739\n",
      "Epoch   13/20 hypothesis: tensor([154.0486, 185.1025, 175.7947, 198.5905, 141.2200]), Cost: 5.842213\n",
      "Epoch   14/20 hypothesis: tensor([154.0485, 185.1022, 175.7957, 198.5899, 141.2201]), Cost: 5.839700\n",
      "Epoch   15/20 hypothesis: tensor([154.0483, 185.1019, 175.7968, 198.5893, 141.2202]), Cost: 5.837173\n",
      "Epoch   16/20 hypothesis: tensor([154.0482, 185.1016, 175.7978, 198.5887, 141.2202]), Cost: 5.834626\n",
      "Epoch   17/20 hypothesis: tensor([154.0480, 185.1013, 175.7989, 198.5881, 141.2203]), Cost: 5.832105\n",
      "Epoch   18/20 hypothesis: tensor([154.0479, 185.1011, 175.7999, 198.5876, 141.2203]), Cost: 5.829581\n",
      "Epoch   19/20 hypothesis: tensor([154.0477, 185.1008, 175.8010, 198.5870, 141.2204]), Cost: 5.827069\n",
      "Epoch   20/20 hypothesis: tensor([154.0476, 185.1005, 175.8020, 198.5864, 141.2205]), Cost: 5.824584\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 20\n",
    "\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    hypothesis = x_train.matmul(W) + b \n",
    "    \n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print('Epoch {:4d}/{} hypothesis: {}, Cost: {:.6f}'.format(\n",
    "        epoch,nb_epochs,hypothesis.squeeze().detach(),cost.item()\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c2ac30",
   "metadata": {},
   "source": [
    "### nn.Module을 상속해서 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37c27d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MultivariateLinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3, 1) # 입력차원 3, 출력차원 1\n",
    "    \n",
    "    # hypothesis 계산은 forward()에서. Gradient 계산은 backward()에서 알아서 해줌\n",
    "    def forward(self, x): \n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47765ce",
   "metadata": {},
   "source": [
    "### pytorch에서 제공하는 cost function\n",
    "- 후에 다음 cost function을 바꿀 때 편리  \n",
    "- cost function을 계산하면서 생기는 버그가 없어 디버깅 편리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f7b2e5",
   "metadata": {},
   "source": [
    "### Full Code with torch.optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c33955f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 hypothesis: tensor([-21.4956, -23.7951, -23.6062, -26.5968, -17.7471]), Cost: 38044.019531\n",
      "Epoch    1/20 hypothesis: tensor([54.0657, 67.0011, 62.5835, 70.8242, 51.5152]), Cost: 12231.599609\n",
      "Epoch    2/20 hypothesis: tensor([ 96.9038, 118.4760, 111.4477, 126.0549,  90.7820]), Cost: 3935.210938\n",
      "Epoch    3/20 hypothesis: tensor([121.1901, 147.6585, 139.1509, 157.3667, 113.0435]), Cost: 1268.660767\n",
      "Epoch    4/20 hypothesis: tensor([134.9588, 164.2027, 154.8572, 175.1180, 125.6641]), Cost: 411.602051\n",
      "Epoch    5/20 hypothesis: tensor([142.7647, 173.5819, 163.7622, 185.1816, 132.8190]), Cost: 136.131683\n",
      "Epoch    6/20 hypothesis: tensor([147.1902, 178.8990, 168.8113, 190.8867, 136.8753]), Cost: 47.590527\n",
      "Epoch    7/20 hypothesis: tensor([149.6993, 181.9132, 171.6743, 194.1208, 139.1748]), Cost: 19.130428\n",
      "Epoch    8/20 hypothesis: tensor([151.1217, 183.6218, 173.2980, 195.9541, 140.4783]), Cost: 9.981029\n",
      "Epoch    9/20 hypothesis: tensor([151.9282, 184.5902, 174.2191, 196.9932, 141.2172]), Cost: 7.038228\n",
      "Epoch   10/20 hypothesis: tensor([152.3855, 185.1390, 174.7419, 197.5820, 141.6361]), Cost: 6.090261\n",
      "Epoch   11/20 hypothesis: tensor([152.6449, 185.4499, 175.0388, 197.9156, 141.8734]), Cost: 5.783494\n",
      "Epoch   12/20 hypothesis: tensor([152.7919, 185.6259, 175.2077, 198.1044, 142.0079]), Cost: 5.682817\n",
      "Epoch   13/20 hypothesis: tensor([152.8754, 185.7255, 175.3040, 198.2113, 142.0840]), Cost: 5.648392\n",
      "Epoch   14/20 hypothesis: tensor([152.9227, 185.7817, 175.3592, 198.2716, 142.1271]), Cost: 5.635244\n",
      "Epoch   15/20 hypothesis: tensor([152.9496, 185.8133, 175.3910, 198.3055, 142.1514]), Cost: 5.628943\n",
      "Epoch   16/20 hypothesis: tensor([152.9649, 185.8310, 175.4096, 198.3245, 142.1650]), Cost: 5.624812\n",
      "Epoch   17/20 hypothesis: tensor([152.9737, 185.8408, 175.4207, 198.3350, 142.1727]), Cost: 5.621442\n",
      "Epoch   18/20 hypothesis: tensor([152.9787, 185.8461, 175.4276, 198.3408, 142.1769]), Cost: 5.618273\n",
      "Epoch   19/20 hypothesis: tensor([152.9816, 185.8489, 175.4320, 198.3438, 142.1792]), Cost: 5.615177\n",
      "Epoch   20/20 hypothesis: tensor([152.9833, 185.8503, 175.4351, 198.3452, 142.1805]), Cost: 5.612128\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "x_train = torch.FloatTensor([[73,80,75],[93,88,93],[89,91,80],[96,98,100],[73,66,70]])\n",
    "y_train = torch.FloatTensor([[152],[185],[180],[196],[142]])\n",
    "\n",
    "# 모델 초기화\n",
    "model = MultivariateLinearRegressionModel()\n",
    "\n",
    "# optimizer 설정\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5)\n",
    "\n",
    "nb_epochs = 20\n",
    "\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    hypothesis = model(x_train)\n",
    "    \n",
    "    cost = F.mse_loss(hypothesis, y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print('Epoch {:4d}/{} hypothesis: {}, Cost: {:.6f}'.format(\n",
    "        epoch,nb_epochs,hypothesis.squeeze().detach(),cost.item()\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd008c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
